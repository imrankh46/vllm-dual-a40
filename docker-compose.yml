version: '3.8'

services:
  vllm-deployment:
    build: .
    container_name: vllm-dual-a40
    runtime: nvidia
    environment:
      # GPU configuration
      - NVIDIA_VISIBLE_DEVICES=all
      
      # API Configuration (change these)
      - API_KEY=qiCwgirvWt4XCA4S2jekdTJruXXmb08K
      
      # Model configuration
      - EMBEDDING_MODEL=Qwen/Qwen3-Embedding-4B
      - LLM_MODEL=openai/gpt-oss-20b
      
      # Port configuration
      - EMBEDDING_PORT=8002
      - LLM_PORT=8000
      
      # Performance tuning
      - VLLM_USE_FLASHINFER_SAMPLER=0
      
    ports:
      - "8000:8000"  # LLM API
      - "8002:8002"  # Embedding API
      
    volumes:
      # Cache models (persists between restarts)
      - huggingface_cache:/root/.cache/huggingface
      
    # Shared memory for PyTorch
    shm_size: '16gb'
    
    # GPU resource allocation
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Auto-restart on failure
    restart: unless-stopped
    
    # Health monitoring
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

volumes:
  huggingface_cache:
    driver: local
